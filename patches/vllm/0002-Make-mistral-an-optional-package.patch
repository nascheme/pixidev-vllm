From 4f6e856b9cfa3c90a30ea9c83524f271e7233e7f Mon Sep 17 00:00:00 2001
From: Neil Schemenauer <nas@arctrix.com>
Date: Thu, 5 Feb 2026 01:42:12 +0000
Subject: [PATCH] Make "mistral" an optional package.

---
 vllm/entrypoints/llm.py                       |  3 +--
 .../openai/chat_completion/serving.py         | 26 ++++++++++---------
 vllm/entrypoints/pooling/score/serving.py     |  3 +--
 vllm/multimodal/processing/context.py         |  4 +---
 vllm/tokenizers/mistral.py                    |  1 +
 vllm/v1/engine/input_processor.py             |  7 +++--
 vllm/v1/structured_output/backend_xgrammar.py |  3 +--
 7 files changed, 22 insertions(+), 25 deletions(-)

diff --git a/vllm/entrypoints/llm.py b/vllm/entrypoints/llm.py
index 9ef7af098..eca09a164 100644
--- a/vllm/entrypoints/llm.py
+++ b/vllm/entrypoints/llm.py
@@ -70,7 +70,6 @@ from vllm.pooling_params import PoolingParams
 from vllm.sampling_params import BeamSearchParams, RequestOutputKind, SamplingParams
 from vllm.tasks import PoolingTask
 from vllm.tokenizers import TokenizerLike
-from vllm.tokenizers.mistral import MistralTokenizer
 from vllm.usage.usage_lib import UsageContext
 from vllm.utils.collection_utils import as_iter, is_list_of
 from vllm.utils.counter import Counter
@@ -1253,7 +1252,7 @@ class LLM:
     ) -> list[ScoringRequestOutput]:
         model_config = self.model_config
 
-        if isinstance(tokenizer, MistralTokenizer):
+        if hasattr(tokenizer, 'is_mistral'):
             raise ValueError("Score API is not supported for Mistral tokenizer")
 
         if len(data_1) == 1:
diff --git a/vllm/entrypoints/openai/chat_completion/serving.py b/vllm/entrypoints/openai/chat_completion/serving.py
index 265cee554..99b27e582 100644
--- a/vllm/entrypoints/openai/chat_completion/serving.py
+++ b/vllm/entrypoints/openai/chat_completion/serving.py
@@ -72,14 +72,7 @@ from vllm.logprobs import Logprob
 from vllm.outputs import CompletionOutput, RequestOutput
 from vllm.sampling_params import BeamSearchParams, SamplingParams
 from vllm.tokenizers import TokenizerLike
-from vllm.tokenizers.mistral import (
-    MistralTokenizer,
-    maybe_serialize_tool_calls,
-    truncate_tool_call_ids,
-    validate_request_params,
-)
 from vllm.tool_parsers import ToolParser
-from vllm.tool_parsers.mistral_tool_parser import MistralToolCall
 from vllm.tool_parsers.utils import partial_json_loads
 from vllm.utils.collection_utils import as_list
 from vllm.v1.sample.logits_processor import validate_logits_processors_parameters
@@ -243,10 +236,16 @@ class OpenAIServingChat(OpenAIServing):
 
             tool_parser = self.tool_parser
 
-            if isinstance(tokenizer, MistralTokenizer):
+            if hasattr(tokenizer, "is_mistral"):
                 # because of issues with pydantic we need to potentially
                 # re-serialize the tool_calls field of the request
                 # for more info: see comment in `maybe_serialize_tool_calls`
+                from vllm.tokenizers.mistral import (
+                    maybe_serialize_tool_calls,
+                    truncate_tool_call_ids,
+                    validate_request_params,
+                )
+
                 maybe_serialize_tool_calls(request)
                 truncate_tool_call_ids(request)
                 validate_request_params(request)
@@ -254,7 +253,7 @@ class OpenAIServingChat(OpenAIServing):
             # Check if tool parsing is unavailable (common condition)
             tool_parsing_unavailable = (
                 tool_parser is None
-                and not isinstance(tokenizer, MistralTokenizer)
+                and not hasattr(tokenizer, "is_mistral")
                 and not self.use_harmony
             )
 
@@ -1521,9 +1520,12 @@ class OpenAIServingChat(OpenAIServing):
                 enable_auto_tools=self.enable_auto_tools,
                 tool_parser_cls=self.tool_parser,
             )
-            tool_call_class = (
-                MistralToolCall if isinstance(tokenizer, MistralTokenizer) else ToolCall
-            )
+            if hasattr(tokenizer, "is_mistral"):
+                from vllm.tool_parsers.mistral_tool_parser import MistralToolCall
+
+                tool_call_class = MistralToolCall
+            else:
+                tool_call_class = ToolCall
             if (not self.enable_auto_tools or not self.tool_parser) and (
                 not isinstance(request.tool_choice, ChatCompletionNamedToolChoiceParam)
                 and request.tool_choice != "required"
diff --git a/vllm/entrypoints/pooling/score/serving.py b/vllm/entrypoints/pooling/score/serving.py
index 85c74e5a2..be666b13a 100644
--- a/vllm/entrypoints/pooling/score/serving.py
+++ b/vllm/entrypoints/pooling/score/serving.py
@@ -40,7 +40,6 @@ from vllm.logger import init_logger
 from vllm.lora.request import LoRARequest
 from vllm.outputs import PoolingRequestOutput, ScoringRequestOutput
 from vllm.tokenizers import TokenizerLike
-from vllm.tokenizers.mistral import MistralTokenizer
 from vllm.utils.async_utils import make_async, merge_async_iterators
 
 logger = init_logger(__name__)
@@ -199,7 +198,7 @@ class ServingScores(OpenAIServing):
         if len(data_1) == 1:
             data_1 = data_1 * len(data_2)
 
-        if isinstance(tokenizer, MistralTokenizer):
+        if hasattr(tokenizer, "is_mistral"):
             raise ValueError("MistralTokenizer not supported for cross-encoding")
 
         tokenization_kwargs = tokenization_kwargs or {}
diff --git a/vllm/tokenizers/mistral.py b/vllm/tokenizers/mistral.py
index bb85052db..e676dfc51 100644
--- a/vllm/tokenizers/mistral.py
+++ b/vllm/tokenizers/mistral.py
@@ -240,6 +240,7 @@ class MistralTokenizer(TokenizerLike):
     def __init__(self, tokenizer: "MistralCommonBackend") -> None:
         super().__init__()
 
+        self.is_mistral = True
         self.transformers_tokenizer = tokenizer
         self.mistral = tokenizer.tokenizer
         self.instruct = self.mistral.instruct_tokenizer
diff --git a/vllm/v1/engine/input_processor.py b/vllm/v1/engine/input_processor.py
index f7f1608ec..5bea73967 100644
--- a/vllm/v1/engine/input_processor.py
+++ b/vllm/v1/engine/input_processor.py
@@ -32,7 +32,6 @@ from vllm.pooling_params import PoolingParams
 from vllm.renderers import RendererLike
 from vllm.sampling_params import _SAMPLING_EPS, SamplingParams
 from vllm.tokenizers import TokenizerLike
-from vllm.tokenizers.mistral import MistralTokenizer
 from vllm.utils import length_from_prompt_token_ids_or_embeds, random_uuid
 from vllm.utils.torch_utils import set_default_torch_num_threads
 from vllm.v1.engine import EngineCoreRequest
@@ -344,7 +343,7 @@ class InputProcessor:
             # allows <|special_token|> and similar, see
             # https://github.com/guidance-ai/llguidance/blob/main/docs/syntax.md#special-tokens
             # Without tokenizer these are disallowed in grammars.
-            if isinstance(self.tokenizer, MistralTokenizer):
+            if hasattr(self.tokenizer, 'is_mistral'):
                 raise ValueError(
                     "Mistral tokenizer is not supported for the 'guidance' "
                     "structured output backend. Please use ['xgrammar', 'outlines'] "
@@ -356,7 +355,7 @@ class InputProcessor:
             validate_structured_output_request_outlines(params)
         elif backend == "lm-format-enforcer":
             # lm format enforcer backend
-            if isinstance(self.tokenizer, MistralTokenizer):
+            if hasattr(self.tokenizer, 'is_mistral'):
                 raise ValueError(
                     "Mistral tokenizer is not supported for the 'lm-format-enforcer' "
                     "structured output backend. Please use ['xgrammar', 'outlines'] "
@@ -390,7 +389,7 @@ class InputProcessor:
                         schema = so_params.json
                     skip_guidance = has_guidance_unsupported_json_features(schema)
 
-                if isinstance(self.tokenizer, MistralTokenizer) or skip_guidance:
+                if hasattr(self.tokenizer, 'is_mistral') or skip_guidance:
                     # Fall back to outlines if the tokenizer is Mistral
                     # or if schema contains features unsupported by guidance
                     validate_structured_output_request_outlines(params)
diff --git a/vllm/v1/structured_output/backend_xgrammar.py b/vllm/v1/structured_output/backend_xgrammar.py
index 617132577..c3057f92f 100644
--- a/vllm/v1/structured_output/backend_xgrammar.py
+++ b/vllm/v1/structured_output/backend_xgrammar.py
@@ -11,7 +11,6 @@ import vllm.envs
 from vllm.logger import init_logger
 from vllm.sampling_params import SamplingParams
 from vllm.tokenizers.deepseek_v32 import DeepseekV32Tokenizer
-from vllm.tokenizers.mistral import MistralTokenizer
 from vllm.utils.import_utils import LazyLoader
 from vllm.v1.structured_output.backend_types import (
     StructuredOutputBackend,
@@ -39,7 +38,7 @@ class XgrammarBackend(StructuredOutputBackend):
             self.vllm_config.structured_outputs_config.disable_any_whitespace
         )
 
-        if isinstance(self.tokenizer, MistralTokenizer):
+        if hasattr(self.tokenizer, 'is_mistral'):
             # NOTE: ideally, xgrammar should handle this accordingly.
             # refer to https://github.com/mlc-ai/xgrammar/blob/d77c0a0173ef14779c918e3be7966ba852f7910f/python/xgrammar/tokenizer_info.py#L98
             stop_token_ids = [self.tokenizer.eos_token_id]
diff --git a/vllm/multimodal/processing/context.py b/vllm/multimodal/processing/context.py
index d4894a984..3604c15b1 100644
--- a/vllm/multimodal/processing/context.py
+++ b/vllm/multimodal/processing/context.py
@@ -359,10 +359,8 @@ class InputProcessingContext:

             typ = ProcessorMixin

-        from vllm.tokenizers.mistral import MistralTokenizer
-
         tokenizer = self.tokenizer
-        if isinstance(tokenizer, MistralTokenizer):
+        if hasattr(tokenizer, 'is_mistral'):
             tokenizer = tokenizer.transformers_tokenizer

         return cached_processor_from_config(
--
2.43.0

